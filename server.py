
from fastapi import FastAPI
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
from transformers import BertTokenizerFast, BertForQuestionAnswering
import faiss
import json
import numpy as np
import torch
import random
# ğŸš€ Khá»Ÿi táº¡o FastAPI
app = FastAPI()

# ğŸ“¦ Load dataset
with open("dataset_with_key_answer.json", "r", encoding="utf-8") as f:
    dataset = json.load(f)

# ğŸ§  Load embedding model (máº¡nh)
embed_model = SentenceTransformer('all-mpnet-base-v2')

# ğŸ”¢ Encode + normalize cÃ¡c cÃ¢u há»i trong dataset
questions = [item["question"] for item in dataset]
question_embeddings = embed_model.encode(questions, convert_to_numpy=True, normalize_embeddings=True)

# ğŸ¯ FAISS index (cosine similarity = inner product vÃ¬ Ä‘Ã£ normalize)
index = faiss.IndexFlatIP(question_embeddings.shape[1])
index.add(question_embeddings)
id_to_sample = {i: item for i, item in enumerate(dataset)}

# ğŸ¤– Load model BERT QA
model = BertForQuestionAnswering.from_pretrained("yenly1234/BERTQAENG")
tokenizer = BertTokenizerFast.from_pretrained("yenly1234/BERTQAENG")

# ğŸ“¥ Schema request
class QuestionRequest(BaseModel):
    question: str

# ğŸ’¬ HÃ m xá»­ lÃ½ QA
def chat(query):
    # BÆ°á»›c 1: Encode vÃ  tÃ¬m nearest question
    query_vec = embed_model.encode(query, convert_to_numpy=True, normalize_embeddings=True).reshape(1, -1)
    D, I = index.search(query_vec, 5)

    # Chá»n cÃ¢u há»i giá»‘ng nháº¥t
    best_idx = I[0][0]
    matched = id_to_sample[best_idx]
    if "```" in matched["answer"]:
        context = matched["answer"]
    else:
       context = random.choice(matched["key_answer"])


    # BÆ°á»›c 2: QA tá»« context + query
    inputs = tokenizer(query, context, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)

    start = torch.argmax(outputs.start_logits)
    end = torch.argmax(outputs.end_logits)
    if end < start:
        end = start

    # answer = tokenizer.decode(inputs["input_ids"][0][start:end+1], skip_special_tokens=True)
    answer_ids = inputs["input_ids"][0][start:end+len(context)]
    answer = tokenizer.decode(answer_ids, skip_special_tokens=True)

    # BÆ°á»›c 3: Tráº£ káº¿t quáº£
    return {
        "your_question": query,
        "matched_question": matched["question"],
        "context_used": context,
        "bert_generated_answer": answer,
        "original_answer": matched["answer"],
        "label": matched.get("label", ""),
        "language": matched.get("language", ""),
        "start_char": matched.get("start_char", -1),
        "end_char": matched.get("end_char", -1),
        "key_answer": matched.get("key_answer","")
    }

# ğŸŒ API endpoint
@app.post("/chat")
def chatbot_api(data: QuestionRequest):
    return chat(data.question)

# â–¶ï¸ Cháº¡y server náº¿u gá»i trá»±c tiáº¿p
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
